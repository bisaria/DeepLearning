{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity: Deep Learning using TensorFlow\n",
    "## Assignment 3\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data we generated in 1_notmnist.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "\n",
    "* data as a flat matrix,\n",
    "* labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). The right amount of regularization should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subset = 10000\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  # L2 Regularization\n",
    "  regularizer = tf.nn.l2_loss(weights)\n",
    "  # loss after using L2 Regularization\n",
    "  loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 49.114323\n",
      "Training accuracy: 7.2%\n",
      "Validation accuracy: 11.5%\n",
      "Loss at step 100: 12.039999\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 72.5%\n",
      "Loss at step 200: 4.566074\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 76.9%\n",
      "Loss at step 300: 2.010421\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 79.6%\n",
      "Loss at step 400: 1.139898\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 81.1%\n",
      "Loss at step 500: 0.840110\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 81.7%\n",
      "Loss at step 600: 0.735663\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 81.9%\n",
      "Loss at step 700: 0.698938\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 82.0%\n",
      "Loss at step 800: 0.685928\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For beta = 0.01 : Accuracy Score on Test Set:  88.8%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network with L2 Regularization\n",
    "    * Single Layer\n",
    "    \n",
    "    Here batch_size for training is considerd as 128, which means the training samples (200000 in this exercise) will been randomly divided into 200000/128 batches with 128 samples in each batch. In other words, every complete training round will have 200000/128 steps.\n",
    "    \n",
    "    An epoch is considered as the number of steps when all of the training data has been passed through the learning model once, which will be 200000/128 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 1024\n",
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  relu_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  logits = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  # L2 Regularization\n",
    "  regularizer = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  # loss after using L2 Regularization\n",
    "  loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3513.550781\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 25.3%\n",
      "Minibatch loss at step 500: 21.248220\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1000: 0.948650\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1500: 0.586573\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2000: 0.611937\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2500: 0.710213\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 0.765545\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 3500: 0.767605\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 4000: 0.662750\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 4500: 0.688409\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 5000: 0.697968\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 5500: 0.786071\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 6000: 0.824578\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 6500: 0.617071\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 7000: 0.803146\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 7500: 0.874642\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 8000: 0.930554\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 8500: 0.663493\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 9000: 0.836120\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 9500: 0.740908\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 10000: 0.804774\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural network gives better result as compared to logistic regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "\n",
    "##### Considering only 500 training data, thereby, restricting number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3448.132324\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 500: 21.069828\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1000: 0.490235\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1500: 0.300318\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2000: 0.282700\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2500: 0.281390\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 3000: 0.277934\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 3500: 0.268778\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 4000: 0.268410\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 4500: 0.265331\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 5000: 0.263989\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 5500: 0.263227\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 6000: 0.257293\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 6500: 0.255185\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 7000: 0.256734\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 7500: 0.252498\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 8000: 0.251201\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 8500: 0.255578\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 9000: 0.256471\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 9500: 0.262707\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 10000: 0.262351\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 86.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "train_dataset_2 = train_dataset[:500,:]\n",
    "train_labels_2 = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting is very obvious here with 100% accuracy on training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 1024\n",
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  relu_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  # Drop out in relu layer\n",
    "  keep_prob = tf.placeholder(\"float\")\n",
    "  relu_layer_dropout = tf.nn.dropout(relu_layer, keep_prob)\n",
    "  logits = tf.matmul(relu_layer_dropout, weights_2) + biases_2\n",
    "  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  # L2 Regularization\n",
    "  regularizer = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "  # loss after using L2 Regularization\n",
    "  loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3587.162598\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 23.3%\n",
      "Minibatch loss at step 500: 21.337776\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 1000: 1.044617\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500: 0.652918\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 0.675978\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2500: 0.837159\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 3000: 0.843557\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 3500: 0.830758\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 4000: 0.752345\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 4500: 0.750211\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 5000: 0.734512\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 5500: 0.885577\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 6000: 0.867540\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 6500: 0.692108\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 7000: 0.874965\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 7500: 0.955431\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 8000: 1.011686\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 8500: 0.711539\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 9000: 0.904487\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 9500: 0.828240\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 10000: 0.884038\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                keep_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using dropout improves the model performance on the test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extreme Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3556.819336\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 27.7%\n",
      "Minibatch loss at step 500: 21.111876\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1000: 0.517459\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 1500: 0.335398\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2000: 0.311299\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500: 0.301840\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 3000: 0.296899\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 3500: 0.301349\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 4000: 0.296846\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 4500: 0.286273\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 5000: 0.286915\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 5500: 0.291484\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 6000: 0.274928\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 6500: 0.275068\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 7000: 0.283895\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 7500: 0.269533\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 8000: 0.270013\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 8500: 0.280784\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 9000: 0.273100\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 9500: 0.292555\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 10000: 0.293184\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Test accuracy: 86.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "train_dataset_2 = train_dataset[:500,:]\n",
    "train_labels_2 = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,\n",
    "                keep_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "`global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN Model with 5 Hidden Layers \n",
    "\n",
    "* Model with 5 hidden layers\n",
    "    - RELUs\n",
    "    - Number of nodes in each hidden layer is 50% of that in the previous one\n",
    "* Overfitting measures\n",
    "    - L2 Regularization\n",
    "        - Learning rate with exponential decay; starting value = 0.01\n",
    "    - Dropout\n",
    "* Number of steps : 15,000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_1 = 1024\n",
    "hidden_2 = int(hidden_1 * 0.5)\n",
    "hidden_3 = int(hidden_2 * 0.5)\n",
    "hidden_4 = int(hidden_3 * 0.5)\n",
    "hidden_5 = int(hidden_4 * 0.5)\n",
    "\n",
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "import math\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # Adjusted the stddev value as per https://discussions.udacity.com/t/assignment-2-2-hidden-layers-error/183933/4 \n",
    "  # Note: any stddev that doesn't produce NAN is fine.\n",
    "    \n",
    "  # Hidden RELU Layer 1\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_1], \n",
    "                        stddev=2.0 / math.sqrt(float(image_size * image_size + hidden_1))))\n",
    "  biases_1 = tf.Variable(tf.zeros([hidden_1]))\n",
    "    \n",
    "  # Hidden RELU Layer 2\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_1, hidden_2], \n",
    "                        stddev=2.0 / math.sqrt(float(hidden_1 + hidden_2))))\n",
    "  biases_2 = tf.Variable(tf.zeros([hidden_2]))\n",
    "  \n",
    "  # Hidden RELU Layer 3\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_2, hidden_3], \n",
    "                        stddev=2.0 / math.sqrt(float(hidden_2 + hidden_3))))\n",
    "  biases_3 = tf.Variable(tf.zeros([hidden_3]))\n",
    "   \n",
    "  # Hidden RELU Layer 4\n",
    "  weights_4 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_3, hidden_4], \n",
    "                        stddev=2.0 / math.sqrt(float(hidden_3 + hidden_4))))\n",
    "  biases_4 = tf.Variable(tf.zeros([hidden_4]))\n",
    " \n",
    "    # Hidden RELU Layer 5\n",
    "  weights_5 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_4, hidden_5], \n",
    "                        stddev=2.0 / math.sqrt(float(hidden_4 + hidden_5))))\n",
    "  biases_5 = tf.Variable(tf.zeros([hidden_5]))\n",
    "  \n",
    "  # Outer Layer\n",
    "  weights_6 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_5, num_labels], \n",
    "                        stddev=2.0 / math.sqrt(float(hidden_5 + num_labels))))\n",
    "  biases_6= tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # Hidden RELU Layer 1\n",
    "  relu_layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  # Drop out in relu layer 1\n",
    "  keep_prob = tf.placeholder(\"float\")\n",
    "  relu_layer_dropout_1 = tf.nn.dropout(relu_layer_1, keep_prob)\n",
    "    \n",
    "  # Hidden RELU Layer 2\n",
    "  relu_layer_2 = tf.nn.relu(tf.matmul(relu_layer_dropout_1, weights_2) + biases_2)\n",
    "  # Drop out in relu layer 2\n",
    "  relu_layer_dropout_2 = tf.nn.dropout(relu_layer_2, keep_prob)\n",
    "  \n",
    "  # Hidden RELU Layer 3\n",
    "  relu_layer_3 = tf.nn.relu(tf.matmul(relu_layer_dropout_2, weights_3) + biases_3)\n",
    "  # Drop out in relu layer 3\n",
    "  relu_layer_dropout_3 = tf.nn.dropout(relu_layer_3, keep_prob)\n",
    "  \n",
    "  # Hidden RELU Layer 4\n",
    "  relu_layer_4 = tf.nn.relu(tf.matmul(relu_layer_dropout_3, weights_4) + biases_4)\n",
    "  # Drop out in relu layer 4\n",
    "  relu_layer_dropout_4 = tf.nn.dropout(relu_layer_4, keep_prob)\n",
    " \n",
    "  # Hidden RELU Layer 5\n",
    "  relu_layer_5 = tf.nn.relu(tf.matmul(relu_layer_dropout_4, weights_5) + biases_5)\n",
    "  # Drop out in relu layer 5\n",
    "  relu_layer_dropout_5= tf.nn.dropout(relu_layer_5, keep_prob)\n",
    " \n",
    "  # Outer Layer\n",
    "  logits = tf.matmul(relu_layer_dropout_5, weights_6) + biases_6\n",
    "  \n",
    "  # Normal loss function\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  # L2 Regularization\n",
    "  regularizer = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(\n",
    "      weights_3) + tf.nn.l2_loss(weights_4) + tf.nn.l2_loss(weights_5) + tf.nn.l2_loss(weights_6)\n",
    "  # loss after using L2 Regularization\n",
    "  loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    \n",
    "  # Optimizer.\n",
    "  # Decaying learning rate\n",
    "  # Decay step = no of steps after which learning rate will be updated\n",
    "  # Set up a variable that's incremented once per batch and\n",
    "  # controls the learning rate decay.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  start_learning_rate = 0.01\n",
    "  learning_rate = tf.train.exponential_decay(start_learning_rate, # Base learning rate.\n",
    "                                      global_step,  # global_step: Current index into the dataset. \n",
    "                                      5000,         # Decay step: each epoch here is 200000/128=1562.5 steps\n",
    "                                      0.96,         # Decay rate.\n",
    "                                      staircase=True)\n",
    "  # Note the global_step=global_step parameter to minimize. \n",
    "  # That tells the optimizer to helpfully increment the 'global_step' parameter for you every time it trains.  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, \n",
    "                                                                        global_step=global_step)\n",
    "   \n",
    "  # Predictions for the training\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Predictions for the validation data.\n",
    "  valid_logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "  valid_relu_1 = tf.nn.relu(valid_logits_1)\n",
    "  \n",
    "  valid_logits_2 = tf.matmul(valid_relu_1, weights_2) + biases_2\n",
    "  valid_relu_2 = tf.nn.relu(valid_logits_2)  \n",
    "\n",
    "  valid_logits_3 = tf.matmul(valid_relu_2, weights_3) + biases_3\n",
    "  valid_relu_3 = tf.nn.relu(valid_logits_3)  \n",
    "  \n",
    "  valid_logits_4 = tf.matmul(valid_relu_3, weights_4) + biases_4\n",
    "  valid_relu_4 = tf.nn.relu(valid_logits_4)  \n",
    "  \n",
    "  valid_logits_5 = tf.matmul(valid_relu_4, weights_5) + biases_5\n",
    "  valid_relu_5 = tf.nn.relu(valid_logits_5)  \n",
    "  \n",
    "  valid_logits_6 = tf.matmul(valid_relu_5, weights_6) + biases_6\n",
    "    \n",
    "  valid_prediction = tf.nn.softmax(valid_logits_6)\n",
    "\n",
    "  # Predictions for the test data.\n",
    "  test_logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  test_relu_1 = tf.nn.relu(test_logits_1)\n",
    "  \n",
    "  test_logits_2 = tf.matmul(test_relu_1, weights_2) + biases_2\n",
    "  test_relu_2 = tf.nn.relu(test_logits_2)  \n",
    "\n",
    "  test_logits_3 = tf.matmul(test_relu_2, weights_3) + biases_3\n",
    "  test_relu_3 = tf.nn.relu(test_logits_3)  \n",
    "\n",
    "  test_logits_4 = tf.matmul(test_relu_3, weights_4) + biases_4\n",
    "  test_relu_4 = tf.nn.relu(test_logits_4)  \n",
    "\n",
    "  test_logits_5 = tf.matmul(test_relu_4, weights_5) + biases_5\n",
    "  test_relu_5 = tf.nn.relu(test_logits_5)  \n",
    "\n",
    "  test_logits_6 = tf.matmul(test_relu_5, weights_6) + biases_6  \n",
    "  test_prediction = tf.nn.softmax(test_logits_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6.804766\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 9.3%\n",
      "Minibatch loss at step 500: 4.032598\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 23.6%\n",
      "Minibatch loss at step 1000: 3.750102\n",
      "Minibatch accuracy: 25.8%\n",
      "Validation accuracy: 42.3%\n",
      "Minibatch loss at step 1500: 3.245124\n",
      "Minibatch accuracy: 38.3%\n",
      "Validation accuracy: 58.0%\n",
      "Minibatch loss at step 2000: 3.210968\n",
      "Minibatch accuracy: 43.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 2500: 3.032999\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 3000: 2.735332\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 3500: 2.756519\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 4000: 2.788174\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 4500: 2.515474\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 5000: 2.433222\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 5500: 2.423468\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 6000: 2.527008\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 6500: 2.194419\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 7000: 2.306954\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 7500: 2.527934\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 8000: 2.372506\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 8500: 1.986309\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 9000: 2.191247\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 9500: 2.206452\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 10000: 2.139791\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 10500: 2.114667\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 11000: 2.124454\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 11500: 2.070670\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 12000: 2.280128\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 12500: 2.022823\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 13000: 2.119970\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 13500: 2.081277\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 14000: 1.943875\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 14500: 1.997872\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 15000: 1.845917\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 15500: 1.956824\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 16000: 1.840317\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 16500: 1.730737\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 17000: 1.782844\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 17500: 1.551369\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 18000: 1.747159\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 18500: 1.822943\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 19000: 1.577337\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 19500: 1.810753\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 20000: 1.903370\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 20500: 1.624809\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 21000: 1.806881\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 21500: 1.738962\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 22000: 1.577854\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 22500: 1.641550\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 23000: 1.692333\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 23500: 1.482873\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 24000: 1.685335\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 24500: 1.671486\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 25000: 1.783657\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 25500: 1.589290\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 26000: 1.636632\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 26500: 1.525098\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 27000: 1.561834\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 27500: 1.757689\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 28000: 1.485148\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 28500: 1.483319\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 29000: 1.785676\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 29500: 1.631732\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 30000: 1.524098\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 30500: 1.453226\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 31000: 1.572179\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 31500: 1.488299\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 32000: 1.387465\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 32500: 1.548264\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 33000: 1.546062\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 33500: 1.295459\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 34000: 1.754502\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 34500: 1.656559\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 35000: 1.339179\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 35500: 1.502674\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 36000: 1.497382\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 36500: 1.365980\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 37000: 1.418648\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 37500: 1.158665\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 38000: 1.470743\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 38500: 1.363068\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 39000: 1.560978\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 39500: 1.455091\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 40000: 1.297829\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 40500: 1.431495\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 41000: 1.227817\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 41500: 1.489477\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 42000: 1.267743\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 42500: 1.322699\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 43000: 1.301431\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 43500: 1.302545\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 44000: 1.343225\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 44500: 1.286568\n",
      "Minibatch accuracy: 86.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 45000: 1.310206\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 45500: 1.165762\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 46000: 1.145842\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 46500: 1.536286\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 47000: 1.176394\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 47500: 1.180098\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 48000: 1.207114\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 48500: 1.258222\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 49000: 1.286874\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 49500: 1.235683\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 50000: 1.308735\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 50500: 1.371897\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 51000: 1.143800\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 51500: 1.182097\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 52000: 1.285549\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 52500: 1.125270\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 53000: 1.069888\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 53500: 1.324988\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 54000: 1.287739\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 54500: 1.337905\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 55000: 1.101998\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 55500: 1.287743\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 56000: 1.122588\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 56500: 1.028130\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 57000: 1.072535\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 57500: 1.105749\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 58000: 0.958441\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 58500: 1.017952\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 59000: 1.227513\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 59500: 1.037941\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 60000: 1.110142\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 60500: 1.115096\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 61000: 0.944245\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 61500: 1.039468\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 62000: 0.910933\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 62500: 1.207104\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 63000: 1.136097\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 63500: 1.048399\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 64000: 1.040099\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 64500: 1.223176\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 65000: 1.026851\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 65500: 0.997451\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 66000: 1.038661\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 66500: 1.180174\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 67000: 1.021461\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 67500: 0.918733\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 68000: 1.084973\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 68500: 0.938965\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 69000: 0.974708\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 69500: 1.025112\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 70000: 1.090736\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 70500: 0.978542\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 71000: 1.105583\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 71500: 1.014713\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 72000: 0.954982\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 72500: 0.996724\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 73000: 1.063256\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 73500: 1.104174\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 74000: 1.052952\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 74500: 0.833315\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 75000: 1.003028\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 75500: 0.942692\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 76000: 0.908086\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 76500: 0.901587\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 77000: 1.083122\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 77500: 0.960811\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 78000: 0.892756\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 78500: 1.080168\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 79000: 0.849935\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 79500: 1.000796\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 80000: 0.847333\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 80500: 0.967551\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 81000: 0.914809\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 81500: 0.987431\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 82000: 0.864907\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 82500: 0.962669\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 83000: 1.016966\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 83500: 0.861801\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 84000: 0.960644\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 84500: 0.844911\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 85000: 0.910592\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 85500: 0.998953\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 86000: 0.871746\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 86500: 0.940663\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 87000: 0.793929\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 87500: 0.923824\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 88000: 0.846314\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 88500: 0.788783\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 89000: 1.070700\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 89500: 0.955315\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 90000: 0.866157\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 90500: 0.790577\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 91000: 0.904507\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 91500: 0.927198\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 92000: 0.851750\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 92500: 0.868930\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 93000: 0.916286\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 93500: 0.846813\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 94000: 0.904889\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 94500: 0.936953\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 95000: 0.886565\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 95500: 1.068510\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 96000: 1.066453\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 96500: 0.845059\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 97000: 0.714331\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 97500: 0.871103\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 98000: 0.780440\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 98500: 0.735306\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 99000: 0.867080\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 99500: 0.818350\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 100000: 0.832023\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 100500: 0.784346\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 101000: 1.007184\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 101500: 0.748079\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 102000: 0.876657\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 102500: 0.823506\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 103000: 0.866988\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 103500: 0.900799\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 104000: 0.766707\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 104500: 0.978583\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 105000: 0.765852\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 105500: 0.925021\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 106000: 0.951622\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 106500: 0.873775\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 107000: 0.786658\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 107500: 0.881593\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 108000: 0.802791\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 108500: 0.927378\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 109000: 0.922087\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 109500: 0.789224\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 110000: 0.896032\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 110500: 0.830060\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 111000: 1.172267\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 111500: 0.695289\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 112000: 0.768740\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 112500: 0.913301\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 113000: 0.660272\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 113500: 0.892656\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 114000: 0.762000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 114500: 0.945538\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 115000: 0.768693\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 115500: 0.959077\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 116000: 0.725839\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 116500: 0.938509\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 117000: 0.930339\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 117500: 0.728699\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 118000: 0.832689\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 118500: 0.767989\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 119000: 0.737256\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 119500: 0.760673\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 120000: 0.848464\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 120500: 0.820310\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 121000: 0.768708\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 121500: 0.817181\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 122000: 0.787563\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 122500: 0.800010\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 123000: 0.722303\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 123500: 0.680128\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 124000: 0.823754\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 124500: 0.821870\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 125000: 0.869564\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 125500: 0.695707\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 126000: 0.664929\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 126500: 0.809050\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 127000: 0.895863\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 127500: 0.919034\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 128000: 0.840429\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 128500: 0.839153\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 129000: 0.836392\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 129500: 0.790722\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 130000: 0.687479\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 130500: 0.815693\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 131000: 0.627335\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 131500: 0.733021\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 132000: 0.713284\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 132500: 0.794644\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 133000: 0.714505\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 133500: 0.788063\n",
      "Minibatch accuracy: 86.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 134000: 0.872208\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 134500: 0.929759\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 135000: 0.882237\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 135500: 0.718481\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 136000: 0.911437\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 136500: 0.726614\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 137000: 0.759728\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 137500: 0.773656\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 138000: 0.713713\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 138500: 0.744245\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 139000: 0.784047\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 139500: 0.758266\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 140000: 0.651109\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 140500: 0.743109\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 141000: 0.768819\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 141500: 0.940827\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 142000: 0.820918\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 142500: 0.774908\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 143000: 0.759893\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 143500: 0.912810\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 144000: 0.706679\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 144500: 0.814118\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 145000: 0.724268\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 145500: 0.601242\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 146000: 0.834309\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 146500: 0.674266\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 147000: 0.830479\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 147500: 0.634764\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 148000: 0.666868\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 148500: 0.684201\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 149000: 0.795028\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 149500: 0.726545\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 150000: 0.650992\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 150001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                keep_prob : 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minibatch loss at step 150000: 0.650992\n",
    "Minibatch accuracy: 91.4%\n",
    "Validation accuracy: 90.0%\n",
    "Test accuracy: 95.8%**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
